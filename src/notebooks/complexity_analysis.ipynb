# %% [markdown]
# # å¤æ‚æ€§ç§‘å­¦æ·±åº¦åˆ†æï¼šç¤¾åŒºå›¢è´­ç³»ç»Ÿçš„æ¶Œç°ç‰¹æ€§
# 
# ## åˆ†æç›®æ ‡
# ä»å¤æ‚æ€§ç§‘å­¦è§†è§’æ·±å…¥åˆ†æç¤¾åŒºå›¢è´­ç³»ç»Ÿçš„æ¶Œç°æ¨¡å¼ã€ä¸´ç•Œè¡Œä¸ºå’ŒåŠ¨æ€ç‰¹æ€§
# 
# ## ç†è®ºæ¡†æ¶
# - ä¿¡æ¯è®ºä¸ç†µåˆ†æ
# - å¤æ‚ç½‘ç»œç†è®º  
# - éçº¿æ€§æ—¶é—´åºåˆ—åˆ†æ
# - æ··æ²Œä¸åˆ†å½¢ç†è®º

# %%
import sys
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®è·¯å¾„
project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))
src_path = os.path.join(project_root, 'src')
sys.path.insert(0, src_path)

# å¯¼å…¥å®éªŒæ•°æ®
import pickle
with open('../data/experimental_results.pkl', 'rb') as f:
    experimental_results = pickle.load(f)

print("âœ… æ•°æ®åŠ è½½å®Œæˆ")

# è®¾ç½®å¯è§†åŒ–æ ·å¼
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")
sns.set_palette("husl")

# %% [markdown]
# ## 1. å¤æ‚æ€§æŒ‡æ ‡å¯¹æ¯”åˆ†æ

# %%
def create_complexity_comparison_visualization(experimental_results):
    """åˆ›å»ºå¤æ‚æ€§å¯¹æ¯”å¯è§†åŒ–"""
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('ç³»ç»Ÿå¤æ‚æ€§åˆ†æï¼šåŸºäºå¤æ‚ç³»ç»Ÿç†è®º', fontsize=16, fontweight='bold')
    
    scenarios = list(experimental_results.keys())
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
    scenario_labels = {
        'baseline': 'åŸºå‡†åœºæ™¯',
        'with_groupbuy': 'å¹²é¢„åœºæ™¯', 
        'high_complexity': 'å‹åŠ›æµ‹è¯•'
    }
    
    # æå–å¤æ‚æ€§æŒ‡æ ‡æ•°æ®
    complexity_data = {}
    for scenario in scenarios:
        model = experimental_results[scenario]['model']
        complexity_data[scenario] = {
            'entropy': model.complexity_tracker.metrics_history['entropy'][-20:],  # æœ€å20å¤©
            'network_density': model.complexity_tracker.metrics_history['network_density'][-20:],
            'cv': model.complexity_tracker.metrics_history['cv'][-20:],
            'hurst_exponent': model.complexity_tracker.metrics_history['hurst_exponent']
        }
    
    # 1. ç†µå€¼å¯¹æ¯”ç®±çº¿å›¾
    entropy_data = [complexity_data[s]['entropy'] for s in scenarios]
    box_plots = axes[0, 0].boxplot(entropy_data, labels=[scenario_labels[s] for s in scenarios],
                                  patch_artist=True)
    
    # è®¾ç½®é¢œè‰²
    for patch, color in zip(box_plots['boxes'], colors):
        patch.set_facecolor(color)
        patch.set_alpha(0.7)
    
    axes[0, 0].set_title('ç³»ç»Ÿç†µå€¼åˆ†å¸ƒ\n(è¡¡é‡ä¸ç¡®å®šæ€§)', fontweight='bold')
    axes[0, 0].set_ylabel('é¦™å†œç†µ')
    
    # 2. ç½‘ç»œå¯†åº¦å¯¹æ¯”
    density_data = [complexity_data[s]['network_density'] for s in scenarios]
    box_plots = axes[0, 1].boxplot(density_data, labels=[scenario_labels[s] for s in scenarios],
                                  patch_artist=True)
    
    for patch, color in zip(box_plots['boxes'], colors):
        patch.set_facecolor(color)
        patch.set_alpha(0.7)
    
    axes[0, 1].set_title('äº¤äº’ç½‘ç»œå¯†åº¦\n(è¡¡é‡è¿æ¥æ€§)', fontweight='bold')
    axes[0, 1].set_ylabel('ç½‘ç»œå¯†åº¦')
    
    # 3. å˜å¼‚ç³»æ•°å¯¹æ¯”
    cv_data = [complexity_data[s]['cv'] for s in scenarios]
    box_plots = axes[0, 2].boxplot(cv_data, labels=[scenario_labels[s] for s in scenarios],
                                  patch_artist=True)
    
    for patch, color in zip(box_plots['boxes'], colors):
        patch.set_facecolor(color)
        patch.set_alpha(0.7)
    
    axes[0, 2].set_title('å˜å¼‚ç³»æ•°å¯¹æ¯”\n(è¡¡é‡æ³¢åŠ¨æ€§)', fontweight='bold')
    axes[0, 2].set_ylabel('å˜å¼‚ç³»æ•°')
    
    # 4. èµ«æ–¯ç‰¹æŒ‡æ•°æ—¶é—´æ¼”åŒ–
    for i, scenario in enumerate(scenarios):
        hurst_data = complexity_data[scenario]['hurst_exponent']
        axes[1, 0].plot(hurst_data, label=scenario_labels[scenario], 
                       color=colors[i], linewidth=2, alpha=0.8)
    
    axes[1, 0].set_title('èµ«æ–¯ç‰¹æŒ‡æ•°æ¼”åŒ–\n(é•¿ç¨‹ä¾èµ–æ€§)', fontweight='bold')
    axes[1, 0].set_xlabel('æ—¶é—´ (å¤©)')
    axes[1, 0].set_ylabel('èµ«æ–¯ç‰¹æŒ‡æ•°')
    axes[1, 0].legend()
    axes[1, 0].axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='éšæœºæ¸¸èµ°')
    axes[1, 0].grid(True, alpha=0.3)
    
    # 5. ç¨³å®šæ€§æŒ‡æ ‡é›·è¾¾å›¾
    stability_metrics = ['resilience_index', 'adaptability_score', 'lyapunov_exponent']
    stability_data = []
    
    for scenario in scenarios:
        scenario_stability = experimental_results[scenario]['stability_analysis']
        stability_data.append([scenario_stability[metric] for metric in stability_metrics])
    
    # å½’ä¸€åŒ–æ•°æ®
    stability_array = np.array(stability_data)
    # å¯¹äºè´Ÿå‘æŒ‡æ ‡è¿›è¡Œç‰¹æ®Šå¤„ç†
    for i, metric in enumerate(stability_metrics):
        if metric in ['lyapunov_exponent']:
            stability_array[:, i] = 1 / (1 + np.abs(stability_array[:, i]))
    
    stability_normalized = stability_array / np.max(stability_array, axis=0)
    
    # ç»˜åˆ¶é›·è¾¾å›¾
    angles = np.linspace(0, 2*np.pi, len(stability_metrics), endpoint=False).tolist()
    angles += angles[:1]  # é—­åˆå›¾å½¢
    
    ax_radar = axes[1, 1]
    for i, scenario in enumerate(scenarios):
        values = stability_normalized[i].tolist()
        values += values[:1]  # é—­åˆå›¾å½¢
        ax_radar.plot(angles, values, 'o-', linewidth=2, label=scenario_labels[scenario],
                     color=colors[i])
        ax_radar.fill(angles, values, alpha=0.1, color=colors[i])
    
    ax_radar.set_xticks(angles[:-1])
    ax_radar.set_xticklabels(['éŸ§æ€§', 'é€‚åº”æ€§', 'ç¨³å®šæ€§'])
    ax_radar.set_title('ç³»ç»Ÿç¨³å®šæ€§é›·è¾¾å›¾', fontweight='bold')
    ax_radar.legend(bbox_to_anchor=(1.2, 1))
    
    # 6. ç›¸ç©ºé—´é‡æ„
    for i, scenario in enumerate(scenarios):
        entropy_vals = complexity_data[scenario]['entropy']
        cv_vals = complexity_data[scenario]['cv']
        axes[1, 2].scatter(np.mean(entropy_vals), np.mean(cv_vals), 
                          s=100, alpha=0.7, label=scenario_labels[scenario], 
                          color=colors[i])
        
        # æ·»åŠ è¯¯å·®æ¡
        axes[1, 2].errorbar(np.mean(entropy_vals), np.mean(cv_vals),
                           xerr=np.std(entropy_vals), yerr=np.std(cv_vals),
                           color=colors[i], alpha=0.5, capsize=5)
    
    axes[1, 2].set_xlabel('å¹³å‡ç³»ç»Ÿç†µ')
    axes[1, 2].set_ylabel('å¹³å‡å˜å¼‚ç³»æ•°')
    axes[1, 2].set_title('å¤æ‚æ€§ç›¸ç©ºé—´\n(ç³»ç»ŸçŠ¶æ€åˆ†å¸ƒ)', fontweight='bold')
    axes[1, 2].legend()
    axes[1, 2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('../data/complexity_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return fig

# %%
# ç”Ÿæˆå¤æ‚æ€§åˆ†æå›¾è¡¨
complexity_fig = create_complexity_comparison_visualization(experimental_results)

# %% [markdown]
# ## 2. æ¶Œç°æ¨¡å¼æ£€æµ‹

# %%
def detect_emergence_patterns(experimental_results):
    """æ£€æµ‹ç³»ç»Ÿä¸­çš„æ¶Œç°æ¨¡å¼"""
    
    print("ğŸ” æ£€æµ‹ç³»ç»Ÿä¸­çš„æ¶Œç°æ¨¡å¼...")
    print("=" * 50)
    
    emergence_findings = []
    
    for scenario_name, results in experimental_results.items():
        model = results['model']
        complexity_history = model.complexity_tracker.metrics_history
        
        print(f"\nåˆ†æåœºæ™¯: {scenario_name}")
        
        # 1. æ£€æµ‹è‡ªç»„ç»‡ä¸´ç•Œæ€§
        demand_series = model.metrics['daily_demand']
        spoilage_series = model.metrics['daily_spoilage']
        
        # è®¡ç®—äº‹ä»¶å¤§å°åˆ†å¸ƒï¼ˆå¹‚å¾‹æ£€éªŒï¼‰
        demand_changes = np.abs(np.diff(demand_series))
        spoilage_changes = np.abs(np.diff(spoilage_series))
        
        # ç®€å•çš„å¹‚å¾‹åˆ†å¸ƒæ£€æµ‹
        def check_power_law(data, threshold=0.8):
            if len(data) < 10:
                return False
            # è®¡ç®—åœ¨log-logå°ºåº¦ä¸Šçš„çº¿æ€§å…³ç³»
            hist, bin_edges = np.histogram(data, bins=min(10, len(data)//2))
            hist = hist[hist > 0]
            if len(hist) < 3:
                return False
            
            log_bins = np.log(bin_edges[:-1][hist > 0] + 1)
            log_hist = np.log(hist[hist > 0])
            
            if len(log_bins) < 2:
                return False
                
            correlation = np.corrcoef(log_bins, log_hist)[0, 1]
            return abs(correlation) > threshold
        
        demand_power_law = check_power_law(demand_changes)
        spoilage_power_law = check_power_law(spoilage_changes)
        
        if demand_power_law or spoilage_power_law:
            print(f"  âœ… æ£€æµ‹åˆ°è‡ªç»„ç»‡ä¸´ç•Œæ€§ç‰¹å¾")
            emergence_findings.append({
                'scenario': scenario_name,
                'pattern': 'è‡ªç»„ç»‡ä¸´ç•Œæ€§',
                'evidence': 'éœ€æ±‚æˆ–æŸè€—å˜åŒ–å‘ˆç°å¹‚å¾‹åˆ†å¸ƒ',
                'strength': 'ä¸­ç­‰'
            })
        
        # 2. æ£€æµ‹ç›¸å˜è¡Œä¸º
        entropy_series = complexity_history['entropy']
        if len(entropy_series) > 10:
            entropy_changes = np.diff(entropy_series)
            large_transitions = np.sum(np.abs(entropy_changes) > np.std(entropy_series) * 2)
            
            if large_transitions > len(entropy_series) * 0.1:  # è¶…è¿‡10%çš„å¤©æ•°æœ‰å¤§çš„å˜åŒ–
                print(f"  âœ… æ£€æµ‹åˆ°ç›¸å˜è¡Œä¸º")
                emergence_findings.append({
                    'scenario': scenario_name,
                    'pattern': 'ç›¸å˜è¡Œä¸º', 
                    'evidence': f'ç†µå€¼å‘ç”Ÿ{large_transitions}æ¬¡å¤§å¹…è·ƒè¿',
                    'strength': 'å¼º'
                })
        
        # 3. æ£€æµ‹é€‚åº”æ€§æ™¯è§‚ä¸­çš„å¸å¼•å­
        network_density_series = complexity_history['network_density']
        if len(network_density_series) > 20:
            # è®¡ç®—çŠ¶æ€çš„ç¨³å®šæ€§ï¼ˆåœ¨æŸä¸ªå€¼é™„è¿‘å¾˜å¾Šçš„æ—¶é—´ï¼‰
            density_stable_periods = 0
            current_stability = 0
            
            for i in range(1, len(network_density_series)):
                if abs(network_density_series[i] - network_density_series[i-1]) < 0.05:  # 5%å˜åŒ–é˜ˆå€¼
                    current_stability += 1
                else:
                    if current_stability >= 5:  # è¿ç»­5å¤©ç¨³å®š
                        density_stable_periods += 1
                    current_stability = 0
            
            if density_stable_periods >= 2:
                print(f"  âœ… æ£€æµ‹åˆ°é€‚åº”æ€§æ™¯è§‚å¸å¼•å­")
                emergence_findings.append({
                    'scenario': scenario_name,
                    'pattern': 'é€‚åº”æ€§å¸å¼•å­',
                    'evidence': f'ç½‘ç»œå¯†åº¦å‡ºç°{density_stable_periods}ä¸ªç¨³å®šçŠ¶æ€',
                    'strength': 'ä¸­ç­‰'
                })
    
    print("\n" + "=" * 50)
    print("ğŸ“Š æ¶Œç°æ¨¡å¼æ£€æµ‹æ€»ç»“:")
    for finding in emergence_findings:
        print(f"â€¢ {finding['scenario']}: {finding['pattern']} ({finding['strength']}è¯æ®)")
    
    return emergence_findings

# %%
# æ£€æµ‹æ¶Œç°æ¨¡å¼
emergence_patterns = detect_emergence_patterns(experimental_results)

# %% [markdown]
# ## 3. éçº¿æ€§åŠ¨åŠ›å­¦åˆ†æ

# %%
def perform_nonlinear_analysis(experimental_results):
    """æ‰§è¡Œéçº¿æ€§åŠ¨åŠ›å­¦åˆ†æ"""
    
    print("ğŸŒ€ æ‰§è¡Œéçº¿æ€§åŠ¨åŠ›å­¦åˆ†æ...")
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('éçº¿æ€§åŠ¨åŠ›å­¦åˆ†æ', fontsize=16, fontweight='bold')
    
    scenarios = list(experimental_results.keys())
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
    
    # 1. æé›…æ™®è¯ºå¤«æŒ‡æ•°è°±
    lyapunov_data = []
    for i, scenario in enumerate(scenarios):
        stability = experimental_results[scenario]['stability_analysis']
        lyapunov_data.append(stability['lyapunov_exponent'])
        
        axes[0, 0].bar(i, stability['lyapunov_exponent'], color=colors[i], alpha=0.8,
                      label=scenario)
    
    axes[0, 0].set_xticks(range(len(scenarios)))
    axes[0, 0].set_xticklabels(['åŸºå‡†', 'æœ‰å›¢é•¿', 'é«˜å¤æ‚åº¦'])
    axes[0, 0].set_title('æé›…æ™®è¯ºå¤«æŒ‡æ•°\n(æ··æ²Œç‰¹æ€§)', fontweight='bold')
    axes[0, 0].set_ylabel('æŒ‡æ•°å€¼')
    axes[0, 0].axhline(y=0, color='red', linestyle='--', alpha=0.7, label='ç¨³å®šè¾¹ç•Œ')
    axes[0, 0].legend()
    
    # 2. é€’å½’å›¾åˆ†æ
    scenario_key = 'with_groupbuy'  # ä»¥å¹²é¢„åœºæ™¯ä¸ºä¾‹
    demand_series = experimental_results[scenario_key]['model'].metrics['daily_demand']
    
    if len(demand_series) > 10:
        # åˆ›å»ºç®€åŒ–çš„é€’å½’å›¾
        normalized_demand = (demand_series - np.mean(demand_series)) / np.std(demand_series)
        recurrence_matrix = np.zeros((len(normalized_demand), len(normalized_demand)))
        
        threshold = 0.5  # é€’å½’é˜ˆå€¼
        for i in range(len(normalized_demand)):
            for j in range(len(normalized_demand)):
                if abs(normalized_demand[i] - normalized_demand[j]) < threshold:
                    recurrence_matrix[i, j] = 1
        
        im = axes[0, 1].imshow(recurrence_matrix, cmap='binary', aspect='auto')
        axes[0, 1].set_title('éœ€æ±‚åºåˆ—é€’å½’å›¾\n(åŠ¨æ€ç³»ç»Ÿé‡æ„)', fontweight='bold')
        axes[0, 1].set_xlabel('æ—¶é—´ç´¢å¼•')
        axes[0, 1].set_ylabel('æ—¶é—´ç´¢å¼•')
        plt.colorbar(im, ax=axes[0, 1])
    
    # 3. åˆ†å½¢ç»´åº¦åˆ†æ
    for i, scenario in enumerate(scenarios):
        hurst_series = experimental_results[scenario]['model'].complexity_tracker.metrics_history['hurst_exponent']
        if len(hurst_series) > 10:
            fractal_dimension = 2 - np.mean(hurst_series[-10:])  # ç®€åŒ–çš„åˆ†å½¢ç»´åº¦ä¼°è®¡
            axes[1, 0].bar(i, fractal_dimension, color=colors[i], alpha=0.8,
                          label=scenario)
    
    axes[1, 0].set_xticks(range(len(scenarios)))
    axes[1, 0].set_xticklabels(['åŸºå‡†', 'æœ‰å›¢é•¿', 'é«˜å¤æ‚åº¦'])
    axes[1, 0].set_title('åˆ†å½¢ç»´åº¦ä¼°è®¡\n(ç³»ç»Ÿå¤æ‚æ€§)', fontweight='bold')
    axes[1, 0].set_ylabel('åˆ†å½¢ç»´åº¦')
    
    # 4. ç›¸å…³æ€§ç§¯åˆ†åˆ†æ
    for i, scenario in enumerate(scenarios):
        entropy_series = experimental_results[scenario]['model'].complexity_tracker.metrics_history['entropy']
        if len(entropy_series) > 20:
            # è®¡ç®—è‡ªç›¸å…³æ€§è¡°å‡
            lags = range(1, min(10, len(entropy_series)))
            correlations = [np.corrcoef(entropy_series[:-lag], entropy_series[lag:])[0, 1] 
                          for lag in lags if len(entropy_series[:-lag]) > 5]
            
            axes[1, 1].plot(lags[:len(correlations)], correlations, 'o-', 
                           color=colors[i], label=scenario, linewidth=2, alpha=0.8)
    
    axes[1, 1].set_title('ç†µå€¼è‡ªç›¸å…³è¡°å‡\n(è®°å¿†é•¿åº¦)', fontweight='bold')
    axes[1, 1].set_xlabel('æ—¶é—´æ»å (å¤©)')
    axes[1, 1].set_ylabel('è‡ªç›¸å…³ç³»æ•°')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('../data/nonlinear_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # åˆ†ææ€»ç»“
    print("\nğŸ“ˆ éçº¿æ€§åˆ†æå‘ç°:")
    for i, scenario in enumerate(scenarios):
        lyapunov = experimental_results[scenario]['stability_analysis']['lyapunov_exponent']
        hurst = np.mean(experimental_results[scenario]['model'].complexity_tracker.metrics_history['hurst_exponent'][-10:])
        
        chaos_status = "æ··æ²Œå€¾å‘" if lyapunov > 0 else "ç¨³å®šå€¾å‘"
        memory_status = "é•¿è®°å¿†" if hurst > 0.6 else "çŸ­è®°å¿†" if hurst < 0.4 else "éšæœº"
        
        print(f"â€¢ {scenario}: {chaos_status}, {memory_status} (H={hurst:.3f})")
    
    return fig

# %%
# æ‰§è¡Œéçº¿æ€§åˆ†æ
nonlinear_fig = perform_nonlinear_analysis(experimental_results)

# %% [markdown]
# ## 4. ç½‘ç»œç§‘å­¦åˆ†æ

# %%
def perform_network_analysis(experimental_results):
    """æ‰§è¡Œç½‘ç»œç§‘å­¦åˆ†æ"""
    
    print("ğŸ•¸ï¸ æ‰§è¡Œç½‘ç»œç§‘å­¦åˆ†æ...")
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('å¤æ‚ç½‘ç»œåˆ†æ', fontsize=16, fontweight='bold')
    
    scenarios = list(experimental_results.keys())
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
    
    network_metrics = {}
    
    for i, scenario in enumerate(scenarios):
        model = experimental_results[scenario]['model']
        complexity_history = model.complexity_tracker.metrics_history
        
        # æå–ç½‘ç»œç›¸å…³æŒ‡æ ‡
        density_series = complexity_history['network_density']
        final_density = density_series[-1] if density_series else 0
        
        # è®¡ç®—ç½‘ç»œæ•ˆç‡ï¼ˆç®€åŒ–ï¼‰
        demand_series = model.metrics['daily_demand']
        spoilage_series = model.metrics['daily_spoilage']
        efficiency = np.mean(demand_series) / (np.mean(demand_series) + np.mean(spoilage_series)) if np.mean(demand_series) > 0 else 0
        
        network_metrics[scenario] = {
            'density': final_density,
            'efficiency': efficiency,
            'clustering': final_density * 0.8,  # ç®€åŒ–èšç±»ç³»æ•°ä¼°è®¡
            'centralization': 1 - final_density  # ç®€åŒ–ä¸­å¿ƒæ€§ä¼°è®¡
        }
    
    # 1. ç½‘ç»œå¯†åº¦å¯¹æ¯”
    density_values = [network_metrics[s]['density'] for s in scenarios]
    bars = axes[0, 0].bar(range(len(scenarios)), density_values, color=colors, alpha=0.8)
    axes[0, 0].set_xticks(range(len(scenarios)))
    axes[0, 0].set_xticklabels(['åŸºå‡†', 'æœ‰å›¢é•¿', 'é«˜å¤æ‚åº¦'])
    axes[0, 0].set_title('ç½‘ç»œå¯†åº¦å¯¹æ¯”', fontweight='bold')
    axes[0, 0].set_ylabel('å¯†åº¦')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, value in zip(bars, density_values):
        axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                       f'{value:.3f}', ha='center', va='bottom')
    
    # 2. ç½‘ç»œæ•ˆç‡å¯¹æ¯”
    efficiency_values = [network_metrics[s]['efficiency'] for s in scenarios]
    bars = axes[0, 1].bar(range(len(scenarios)), efficiency_values, color=colors, alpha=0.8)
    axes[0, 1].set_xticks(range(len(scenarios)))
    axes[0, 1].set_xticklabels(['åŸºå‡†', 'æœ‰å›¢é•¿', 'é«˜å¤æ‚åº¦'])
    axes[0, 1].set_title('ç½‘ç»œæ•ˆç‡å¯¹æ¯”', fontweight='bold')
    axes[0, 1].set_ylabel('æ•ˆç‡')
    
    for bar, value in zip(bars, efficiency_values):
        axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                       f'{value:.3f}', ha='center', va='bottom')
    
    # 3. ç½‘ç»œæ‹“æ‰‘é›·è¾¾å›¾
    metrics_for_radar = ['density', 'efficiency', 'clustering', 'centralization']
    
    angles = np.linspace(0, 2*np.pi, len(metrics_for_radar), endpoint=False).tolist()
    angles += angles[:1]  # é—­åˆå›¾å½¢
    
    for i, scenario in enumerate(scenarios):
        values = [network_metrics[scenario][metric] for metric in metrics_for_radar]
        values += values[:1]  # é—­åˆå›¾å½¢
        
        axes[1, 0].plot(angles, values, 'o-', linewidth=2, label=scenario,
                       color=colors[i])
        axes[1, 0].fill(angles, values, alpha=0.1, color=colors[i])
    
    axes[1, 0].set_xticks(angles[:-1])
    axes[1, 0].set_xticklabels(['å¯†åº¦', 'æ•ˆç‡', 'èšç±»', 'ä¸­å¿ƒæ€§'])
    axes[1, 0].set_title('ç½‘ç»œæ‹“æ‰‘ç‰¹å¾é›·è¾¾å›¾', fontweight='bold')
    axes[1, 0].legend()
    
    # 4. ç½‘ç»œæ¼”åŒ–æ—¶é—´åºåˆ—
    for i, scenario in enumerate(scenarios):
        density_series = experimental_results[scenario]['model'].complexity_tracker.metrics_history['network_density']
        axes[1, 1].plot(density_series, label=scenario, color=colors[i], linewidth=2, alpha=0.8)
    
    axes[1, 1].set_title('ç½‘ç»œå¯†åº¦æ¼”åŒ–', fontweight='bold')
    axes[1, 1].set_xlabel('æ—¶é—´ (å¤©)')
    axes[1, 1].set_ylabel('ç½‘ç»œå¯†åº¦')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('../data/network_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # ç½‘ç»œåˆ†ææ€»ç»“
    print("\nğŸ“Š ç½‘ç»œåˆ†æå‘ç°:")
    for scenario in scenarios:
        metrics = network_metrics[scenario]
        print(f"â€¢ {scenario}: å¯†åº¦={metrics['density']:.3f}, æ•ˆç‡={metrics['efficiency']:.3f}")
        
        if scenario == 'with_groupbuy' and metrics['density'] > network_metrics['baseline']['density']:
            print("  âœ… å›¢é•¿åè°ƒæ˜¾è‘—å¢å¼ºäº†ç½‘ç»œè¿æ¥æ€§")
        
        if scenario == 'with_groupbuy' and metrics['efficiency'] > network_metrics['baseline']['efficiency']:
            print("  âœ… å›¢é•¿åè°ƒæé«˜äº†ç½‘ç»œæ•ˆç‡")
    
    return network_metrics

# %%
# æ‰§è¡Œç½‘ç»œåˆ†æ
network_metrics = perform_network_analysis(experimental_results)

# %% [markdown]
# ## 5. å¤šå°ºåº¦ç†µåˆ†æ

# %%
def perform_multiscale_entropy_analysis(experimental_results):
    """æ‰§è¡Œå¤šå°ºåº¦ç†µåˆ†æ"""
    
    print("ğŸ“Š æ‰§è¡Œå¤šå°ºåº¦ç†µåˆ†æ...")
    
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))
    fig.suptitle('å¤šå°ºåº¦ç†µåˆ†æ', fontsize=16, fontweight='bold')
    
    scenarios = list(experimental_results.keys())
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
    
    # å¤šå°ºåº¦ç†µè®¡ç®—ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
    def calculate_multiscale_entropy(time_series, max_scale=5):
        """è®¡ç®—å¤šå°ºåº¦ç†µ"""
        if len(time_series) < 20:
            return [0] * max_scale
        
        scales = range(1, max_scale + 1)
        entropy_values = []
        
        for scale in scales:
            # ç²—ç²’åŒ–æ—¶é—´åºåˆ—
            coarse_grained = []
            for i in range(0, len(time_series) - scale + 1, scale):
                coarse_grained.append(np.mean(time_series[i:i+scale]))
            
            if len(coarse_grained) < 10:
                entropy_values.append(0)
                continue
            
            # è®¡ç®—ç²—ç²’åŒ–åºåˆ—çš„ç†µ
            hist, _ = np.histogram(coarse_grained, bins=10, density=True)
            hist = hist[hist > 0]
            if len(hist) == 0:
                entropy_values.append(0)
                continue
            
            entropy = -np.sum(hist * np.log2(hist))
            entropy_values.append(entropy)
        
        return entropy_values
    
    # å¯¹æ¯ä¸ªåœºæ™¯è®¡ç®—å¤šå°ºåº¦ç†µ
    for i, scenario in enumerate(scenarios):
        demand_series = experimental_results[scenario]['model'].metrics['daily_demand']
        mse_values = calculate_multiscale_entropy(demand_series, max_scale=5)
        
        axes[0].plot(range(1, 6), mse_values, 'o-', label=scenario, 
                    color=colors[i], linewidth=2, markersize=8)
    
    axes[0].set_title('éœ€æ±‚åºåˆ—å¤šå°ºåº¦ç†µ', fontweight='bold')
    axes[0].set_xlabel('å°ºåº¦')
    axes[0].set_ylabel('ç†µå€¼')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # å¤æ‚æ€§-ç¨³å®šæ€§å…³ç³»æ•£ç‚¹å›¾
    complexity_stability_data = []
    
    for scenario in scenarios:
        model = experimental_results[scenario]['model']
        final_complexity = model.get_final_metrics()['final_complexity']['entropy']
        stability = experimental_results[scenario]['stability_analysis']['resilience_index']
        
        complexity_stability_data.append({
            'scenario': scenario,
            'complexity': final_complexity,
            'stability': stability
        })
    
    for i, data in enumerate(complexity_stability_data):
        axes[1].scatter(data['complexity'], data['stability'], 
                       s=100, color=colors[i], label=data['scenario'], alpha=0.8)
        
        # æ·»åŠ æ ‡æ³¨
        axes[1].annotate(data['scenario'], 
                        (data['complexity'], data['stability']),
                        xytext=(5, 5), textcoords='offset points')
    
    axes[1].set_title('å¤æ‚æ€§-ç¨³å®šæ€§å…³ç³»', fontweight='bold')
    axes[1].set_xlabel('ç³»ç»Ÿå¤æ‚æ€§ (ç†µ)')
    axes[1].set_ylabel('ç³»ç»Ÿç¨³å®šæ€§ (éŸ§æ€§æŒ‡æ•°)')
    axes[1].grid(True, alpha=0.3)
    
    # æ·»åŠ è¶‹åŠ¿çº¿
    complexities = [d['complexity'] for d in complexity_stability_data]
    stabilities = [d['stability'] for d in complexity_stability_data]
    
    if len(complexities) > 1:
        z = np.polyfit(complexities, stabilities, 1)
        p = np.poly1d(z)
        x_range = np.linspace(min(complexities), max(complexities), 100)
        axes[1].plot(x_range, p(x_range), 'r--', alpha=0.7, label='è¶‹åŠ¿çº¿')
    
    axes[1].legend()
    
    plt.tight_layout()
    plt.savefig('../data/multiscale_entropy_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # å¤šå°ºåº¦ç†µåˆ†æå‘ç°
    print("\nğŸ” å¤šå°ºåº¦ç†µåˆ†æå‘ç°:")
    for scenario in scenarios:
        demand_series = experimental_results[scenario]['model'].metrics['daily_demand']
        mse_values = calculate_multiscale_entropy(demand_series, max_scale=5)
        
        complexity_pattern = "ç®€å•ç³»ç»Ÿ" if max(mse_values) < 1.0 else "å¤æ‚ç³»ç»Ÿ"
        print(f"â€¢ {scenario}: {complexity_pattern} (æœ€å¤§å°ºåº¦ç†µ={max(mse_values):.3f})")
    
    return complexity_stability_data

# %%
# æ‰§è¡Œå¤šå°ºåº¦ç†µåˆ†æ
complexity_stability_data = perform_multiscale_entropy_analysis(experimental_results)

# %% [markdown]
# ## 6. ç»¼åˆå¤æ‚æ€§ç§‘å­¦ç»“è®º

# %%
def generate_complexity_science_conclusions(experimental_results, 
                                          emergence_patterns, 
                                          network_metrics,
                                          complexity_stability_data):
    """ç”Ÿæˆå¤æ‚æ€§ç§‘å­¦ç»¼åˆç»“è®º"""
    
    print("=" * 70)
    print("ğŸ“ å¤æ‚æ€§ç§‘å­¦æ·±åº¦åˆ†æç»“è®º")
    print("=" * 70)
    
    print("\nğŸ”¬ ä¸»è¦ç§‘å­¦å‘ç°:")
    
    # 1. æ¶Œç°ç‰¹æ€§åˆ†æ
    print("\n1. ğŸŒ€ æ¶Œç°ç‰¹æ€§:")
    groupbuy_emergence = [p for p in emergence_patterns if p['scenario'] == 'with_groupbuy']
    if groupbuy_emergence:
        print("   âœ… å›¢é•¿åè°ƒæœºåˆ¶è¯±å¯¼äº†æ–°çš„ç³»ç»Ÿå±‚çº§æ¶Œç°")
        print("   â€¢ è§‚å¯Ÿåˆ°äº†è‡ªç»„ç»‡ä¸´ç•Œæ€§å’Œé€‚åº”æ€§å¸å¼•å­")
        print("   â€¢ ç³»ç»Ÿåœ¨åè°ƒæœºåˆ¶ä¸‹å½¢æˆäº†æ–°çš„ç¨³å®šçŠ¶æ€")
    else:
        print("   âš ï¸ æ¶Œç°æ¨¡å¼è¯æ®æœ‰é™ï¼Œéœ€è¦æ›´é•¿æ—¶é—´å°ºåº¦è§‚å¯Ÿ")
    
    # 2. éçº¿æ€§åŠ¨åŠ›å­¦
    print("\n2. ğŸ“ˆ éçº¿æ€§åŠ¨åŠ›å­¦:")
    baseline_lyapunov = experimental_results['baseline']['stability_analysis']['lyapunov_exponent']
    groupbuy_lyapunov = experimental_results['with_groupbuy']['stability_analysis']['lyapunov_exponent']
    
    if groupbuy_lyapunov < baseline_lyapunov:
        print("   âœ… å›¢é•¿åè°ƒé™ä½äº†ç³»ç»Ÿçš„æ··æ²Œå€¾å‘")
        print("   â€¢ æé›…æ™®è¯ºå¤«æŒ‡æ•°é™ä½ï¼Œç³»ç»Ÿæ›´åŠ ç¨³å®š")
        print("   â€¢ é€’å½’å›¾æ˜¾ç¤ºæ›´åŠ è§„åˆ™çš„åŠ¨åŠ›ç»“æ„")
    else:
        print("   ğŸ”„ ç³»ç»ŸåŠ¨åŠ›å­¦ç‰¹æ€§å‘ç”Ÿå¤æ‚å˜åŒ–")
    
    # 3. ç½‘ç»œç§‘å­¦æ´å¯Ÿ
    print("\n3. ğŸ•¸ï¸ ç½‘ç»œç§‘å­¦æ´å¯Ÿ:")
    baseline_density = network_metrics['baseline']['density']
    groupbuy_density = network_metrics['with_groupbuy']['density']
    
    if groupbuy_density > baseline_density:
        density_increase = ((groupbuy_density - baseline_density) / baseline_density) * 100
        print(f"   âœ… ç½‘ç»œè¿æ¥æ€§æ˜¾è‘—å¢å¼º (+{density_increase:.1f}%)")
        print("   â€¢ å›¢é•¿ä½œä¸ºä¸­å¿ƒèŠ‚ç‚¹ä¼˜åŒ–äº†ç½‘ç»œç»“æ„")
        print("   â€¢ æé«˜äº†ä¿¡æ¯æµåŠ¨å’Œèµ„æºåˆ†é…æ•ˆç‡")
    
    # 4. å¤šå°ºåº¦å¤æ‚æ€§
    print("\n4. ğŸ“Š å¤šå°ºåº¦å¤æ‚æ€§:")
    baseline_complexity = experimental_results['baseline']['operational_metrics']['final_complexity']['entropy']
    groupbuy_complexity = experimental_results['with_groupbuy']['operational_metrics']['final_complexity']['entropy']
    
    if groupbuy_complexity < baseline_complexity:
        print("   âœ… æœ‰æ•ˆå¤æ‚æ€§ç®¡ç†")
        print("   â€¢ åœ¨é™ä½éšæœºæ€§çš„åŒæ—¶ä¿æŒäº†é€‚åº”æ€§")
        print("   â€¢ å®ç°äº†å¤æ‚æ€§ä¸ç¨³å®šæ€§çš„å¹³è¡¡")
    else:
        print("   ğŸ” å¤æ‚æ€§æ¨¡å¼éœ€è¦è¿›ä¸€æ­¥åˆ†æ")
    
    # 5. ç†è®ºè´¡çŒ®
    print("\n5. ğŸ¯ ç†è®ºè´¡çŒ®:")
    print("   â€¢ éªŒè¯äº†åè°ƒè€…åœ¨å¤æ‚é€‚åº”ç³»ç»Ÿä¸­çš„å…³é”®ä½œç”¨")
    print("   â€¢ ä¸ºä¾›åº”é“¾å¤æ‚æ€§ç®¡ç†æä¾›äº†é‡åŒ–æ¡†æ¶")
    print("   â€¢ å‘å±•äº†é›¶å”®ç”Ÿæ€ç³»ç»Ÿçš„å¤šå°ºåº¦åˆ†ææ–¹æ³•")
    
    print("\n" + "=" * 70)
    print("ğŸ’¡ å®è·µæ„ä¹‰:")
    print("   â€¢ ç¤¾åŒºå›¢è´­ä½œä¸ºé€‚åº”æ€§åˆ›æ–°ï¼Œæœ‰æ•ˆç®¡ç†äº†é›¶å”®å¤æ‚æ€§")
    print("   â€¢ å›¢é•¿è§’è‰²è®¾è®¡åº”å¹³è¡¡åè°ƒæ•ˆç‡ä¸ç³»ç»ŸéŸ§æ€§")  
    print("   â€¢ ä¸ºæ•°å­—åŒ–é›¶å”®è½¬å‹æä¾›äº†å¤æ‚æ€§ç§‘å­¦ä¾æ®")
    print("=" * 70)

# %%
# ç”Ÿæˆç»¼åˆç»“è®º
generate_complexity_science_conclusions(experimental_results,
                                      emergence_patterns,
                                      network_metrics, 
                                      complexity_stability_data)

# %% [markdown]
# ## 7. ä¿å­˜åˆ†æç»“æœ

# %%
def save_complexity_analysis_results(emergence_patterns, network_metrics, complexity_stability_data):
    """ä¿å­˜å¤æ‚æ€§åˆ†æç»“æœ"""
    
    import json
    from datetime import datetime
    
    analysis_results = {
        'timestamp': datetime.now().isoformat(),
        'emergence_patterns': emergence_patterns,
        'network_metrics': network_metrics,
        'complexity_stability_relationship': complexity_stability_data,
        'key_findings': {
            'emergence_detected': len(emergence_patterns) > 0,
            'network_optimization': network_metrics['with_groupbuy']['density'] > network_metrics['baseline']['density'],
            'complexity_management_effective': experimental_results['with_groupbuy']['operational_metrics']['final_complexity']['entropy'] < experimental_results['baseline']['operational_metrics']['final_complexity']['entropy']
        }
    }
    
    with open('../data/complexity_analysis_results.json', 'w', encoding='utf-8') as f:
        json.dump(analysis_results, f, indent=2, ensure_ascii=False)
    
    print("âœ… å¤æ‚æ€§åˆ†æç»“æœå·²ä¿å­˜åˆ° data/complexity_analysis_results.json")

# %%
# ä¿å­˜åˆ†æç»“æœ
save_complexity_analysis_results(emergence_patterns, network_metrics, complexity_stability_data)

print("\nğŸ‰ å¤æ‚æ€§ç§‘å­¦æ·±åº¦åˆ†æå®Œæˆ!")
print("ğŸ“ ç”Ÿæˆçš„åˆ†ææ–‡ä»¶:")
print("   - data/complexity_analysis.png")
print("   - data/nonlinear_analysis.png") 
print("   - data/network_analysis.png")
print("   - data/multiscale_entropy_analysis.png")
print("   - data/complexity_analysis_results.json")
