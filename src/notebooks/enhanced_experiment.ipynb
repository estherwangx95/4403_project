# %% [markdown]
# # å¤æ‚ç³»ç»Ÿå»ºæ¨¡å®éªŒï¼šç¤¾åŒºå›¢è´­å¯¹é›¶å”®ä¾›åº”é“¾ç¨³å®šæ€§çš„å½±å“
# 
# ## å®éªŒç›®æ ‡
# é€šè¿‡æ§åˆ¶å®éªŒéªŒè¯ç¤¾åŒºå›¢è´­å›¢é•¿ä½œä¸ºåè°ƒè€…ï¼Œå¯¹ç³»ç»Ÿå¤æ‚æ€§å’Œç¨³å®šæ€§çš„å½±å“
# 
# ## ç†è®ºä¾æ®
# - å¤æ‚é€‚åº”ç³»ç»Ÿç†è®º (Holland, 1995)
# - NKæ¨¡å‹åœ¨ç­–ç•¥ç©ºé—´çš„åº”ç”¨ (Kauffman, 1993)
# - ä¾›åº”é“¾åè°ƒæœºåˆ¶ (Choi et al., 2001)

# %%
import sys
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))
src_path = os.path.join(project_root, 'src')
sys.path.insert(0, src_path)

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from model.enhanced_minimal_market_abm import EnhancedMinimalMarketABM
from metrics.complexity_metrics import ComplexityMetrics

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œæ ·å¼
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")
sns.set_palette("husl")

print("âœ… ç¯å¢ƒè®¾ç½®å®Œæˆ")

# %% [markdown]
# ## 1. å®éªŒè®¾è®¡
# 
# ### ä¸‰ç§å¯¹æ¯”åœºæ™¯
# 1. **åŸºå‡†åœºæ™¯**ï¼šä¼ ç»Ÿé›¶å”®æ¨¡å¼ï¼ˆæ— å›¢é•¿ï¼‰
# 2. **å¹²é¢„åœºæ™¯**ï¼šå¼•å…¥å›¢é•¿åè°ƒæœºåˆ¶
# 3. **å‹åŠ›æµ‹è¯•**ï¼šé«˜å¤æ‚åº¦ç¯å¢ƒï¼ˆæ›´å¤šå±…æ°‘ï¼‰

# %%
def run_theoretical_validation_experiment():
    """
    è¿è¡Œç†è®ºéªŒè¯å®éªŒ
    è¿”å›åŒ…å«ä¸‰ç§åœºæ™¯ç»“æœçš„å­—å…¸
    """
    scenarios = {
        'baseline': {
            'has_groupbuy': False,
            'num_residents': 50,
            'description': 'ä¼ ç»Ÿé›¶å”®æ¨¡å¼ï¼ˆæ— å›¢é•¿åè°ƒï¼‰'
        },
        'with_groupbuy': {
            'has_groupbuy': True, 
            'num_residents': 50,
            'description': 'ç¤¾åŒºå›¢è´­æ¨¡å¼ï¼ˆæœ‰å›¢é•¿åè°ƒï¼‰'
        },
        'high_complexity': {
            'has_groupbuy': True,
            'num_residents': 100,
            'description': 'é«˜å¤æ‚åº¦ç¯å¢ƒæµ‹è¯•'
        }
    }
    
    experimental_results = {}
    
    for scenario_name, config in scenarios.items():
        print(f"ğŸš€ è¿è¡Œåœºæ™¯: {scenario_name} - {config['description']}")
        print(f"   é…ç½®: å±…æ°‘æ•°={config['num_residents']}, å›¢é•¿æ¨¡å¼={config['has_groupbuy']}")
        
        # åˆå§‹åŒ–æ¨¡å‹
        model = EnhancedMinimalMarketABM(
            num_residents=config['num_residents'],
            has_groupbuy=config['has_groupbuy'],
            random_seed=42  # å›ºå®šéšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§
        )
        
        # è¿è¡Œ90å¤©æ¨¡æ‹Ÿï¼ˆä¸€ä¸ªå­£åº¦ï¼‰
        print("   æ­£åœ¨è¿è¡Œæ¨¡æ‹Ÿ...")
        results = model.run_simulation(days=90)
        
        # æ”¶é›†ç»“æœ
        experimental_results[scenario_name] = {
            'config': config,
            'raw_results': results,
            'model': model,
            'operational_metrics': model.get_final_metrics(),
            'stability_analysis': analyze_system_stability(model),
            'complexity_trends': calculate_complexity_trends(model)
        }
        
        print(f"   âœ… å®Œæˆ! å¹³å‡éœ€æ±‚: {experimental_results[scenario_name]['operational_metrics']['avg_daily_demand']:.2f}")
        print()
    
    return experimental_results

# %%
def analyze_system_stability(model):
    """
    åˆ†æç³»ç»Ÿç¨³å®šæ€§ - åŸºäºå¤æ‚æ€§ç†è®º
    """
    demand_series = model.metrics['daily_demand']
    
    stability_metrics = {
        'lyapunov_exponent': estimate_lyapunov_exponent(demand_series),
        'resilience_index': calculate_system_resilience(demand_series),
        'adaptability_score': measure_adaptability(model.complexity_tracker.metrics_history),
        'volatility': calculate_volatility(demand_series),
        'recovery_time': estimate_recovery_time(demand_series)
    }
    
    return stability_metrics

def estimate_lyapunov_exponent(time_series):
    """
    ä¼°è®¡æé›…æ™®è¯ºå¤«æŒ‡æ•° - è¡¡é‡ç³»ç»Ÿå¯¹åˆå§‹æ¡ä»¶çš„æ•æ„Ÿæ€§
    """
    if len(time_series) < 20:
        return 0
    
    try:
        differences = []
        max_lag = min(10, len(time_series)//2)
        
        for i in range(1, max_lag):
            if i >= len(time_series):
                continue
            # è®¡ç®—ä¸åŒæ—¶é—´æ»åçš„å·®å¼‚
            diff = np.abs(time_series[i:] - time_series[:-i])
            differences.append(np.mean(diff))
        
        if len(differences) < 2:
            return 0
            
        # çº¿æ€§æ‹Ÿåˆæ–œç‡ä½œä¸ºæé›…æ™®è¯ºå¤«æŒ‡æ•°ä¼°è®¡
        x = np.arange(1, len(differences) + 1)
        slope, _ = np.polyfit(x, np.log(np.array(differences) + 1e-10), 1)
        return slope
        
    except Exception as e:
        print(f"æé›…æ™®è¯ºå¤«æŒ‡æ•°è®¡ç®—é”™è¯¯: {e}")
        return 0

def calculate_system_resilience(time_series, window=10):
    """
    è®¡ç®—ç³»ç»ŸéŸ§æ€§æŒ‡æ•° - åŸºäºæ¢å¤èƒ½åŠ›
    """
    if len(time_series) < window * 2:
        return 0
    
    # è®¡ç®—æ»šåŠ¨æ ‡å‡†å·®çš„å˜åŒ–
    rolling_std = pd.Series(time_series).rolling(window=window).std().dropna()
    if len(rolling_std) < 2:
        return 0
    
    # éŸ§æ€§å®šä¹‰ä¸ºæ ‡å‡†å·®è¶‹äºç¨³å®šçš„ç¨‹åº¦
    std_of_std = np.std(rolling_std)
    mean_std = np.mean(rolling_std)
    
    if mean_std == 0:
        return 1  # å®Œç¾ç¨³å®š
    
    resilience = 1 / (1 + std_of_std / mean_std)  # å½’ä¸€åŒ–åˆ°0-1
    return resilience

def measure_adaptability(complexity_history):
    """
    æµ‹é‡ç³»ç»Ÿé€‚åº”æ€§ - åŸºäºå¤æ‚æ€§æŒ‡æ ‡çš„å˜åŒ–æ¨¡å¼
    """
    if not complexity_history or len(complexity_history['entropy']) < 10:
        return 0
    
    # è®¡ç®—ç†µå€¼çš„è‡ªé€‚åº”å˜åŒ–
    entropy_series = complexity_history['entropy']
    
    # é€‚åº”æ€§è¡¨ç°ä¸ºé€‚åº¦çš„å¤æ‚æ€§å˜åŒ–ï¼Œè€Œéæç«¯æ³¢åŠ¨
    entropy_change = np.abs(np.diff(entropy_series))
    optimal_change_threshold = np.median(entropy_change) * 2
    
    # è®¡ç®—åœ¨æœ€ä¼˜èŒƒå›´å†…çš„å˜åŒ–æ¯”ä¾‹
    in_optimal_range = np.sum(entropy_change <= optimal_change_threshold) / len(entropy_change)
    
    return in_optimal_range

def calculate_volatility(time_series):
    """è®¡ç®—æ³¢åŠ¨æ€§"""
    if len(time_series) < 2:
        return 0
    return np.std(time_series) / np.mean(time_series) if np.mean(time_series) != 0 else 0

def estimate_recovery_time(time_series, threshold=0.1):
    """ä¼°è®¡ç³»ç»Ÿä»å†²å‡»ä¸­æ¢å¤çš„æ—¶é—´"""
    if len(time_series) < 10:
        return len(time_series)
    
    # ç®€åŒ–ç‰ˆæœ¬ï¼šè®¡ç®—è¶…è¿‡å¹³å‡é˜ˆå€¼çš„è¿ç»­å¤©æ•°
    mean_val = np.mean(time_series)
    threshold_val = mean_val * threshold
    
    deviations = np.abs(time_series - mean_val) > threshold_val
    recovery_periods = []
    current_period = 0
    
    for dev in deviations:
        if dev:
            current_period += 1
        else:
            if current_period > 0:
                recovery_periods.append(current_period)
                current_period = 0
    
    return np.mean(recovery_periods) if recovery_periods else 0

def calculate_complexity_trends(model):
    """è®¡ç®—å¤æ‚æ€§æŒ‡æ ‡çš„å˜åŒ–è¶‹åŠ¿"""
    complexity_data = model.metrics['system_complexity']
    
    if len(complexity_data) < 10:
        return {}
    
    # è®¡ç®—æ—©æœŸå’Œæ™šæœŸå¹³å‡å€¼
    early_period = complexity_data[:10]
    late_period = complexity_data[-10:]
    
    trends = {}
    metrics_to_track = ['entropy', 'network_density', 'cv', 'hurst_exponent']
    
    for metric in metrics_to_track:
        early_vals = [c.get(metric, 0) for c in early_period]
        late_vals = [c.get(metric, 0) for c in late_period]
        
        if early_vals and late_vals:
            early_mean = np.mean(early_vals)
            late_mean = np.mean(late_vals)
            trends[metric] = {
                'early_mean': early_mean,
                'late_mean': late_mean,
                'trend': late_mean - early_mean,
                'trend_percentage': (late_mean - early_mean) / early_mean if early_mean != 0 else 0
            }
    
    return trends

# %% [markdown]
# ## 2. æ‰§è¡Œå®éªŒ

# %%
print("ğŸ§ª å¼€å§‹ç†è®ºéªŒè¯å®éªŒ...")
print("=" * 50)

# è¿è¡Œå®éªŒ
experimental_results = run_theoretical_validation_experiment()

print("ğŸ‰ å®éªŒå®Œæˆ!")
print("=" * 50)

# %% [markdown]
# ## 3. åŸºç¡€ç»“æœåˆ†æ

# %%
def create_preliminary_analysis(experimental_results):
    """åˆ›å»ºåˆæ­¥åˆ†æå›¾è¡¨"""
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('ç¤¾åŒºå›¢è´­å¯¹é›¶å”®ä¾›åº”é“¾çš„å½±å“ï¼šåŸºç¡€è¿è¥æŒ‡æ ‡', fontsize=16, fontweight='bold')
    
    scenarios = list(experimental_results.keys())
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
    scenario_names = {
        'baseline': 'åŸºå‡†åœºæ™¯\n(æ— å›¢é•¿)',
        'with_groupbuy': 'å¹²é¢„åœºæ™¯\n(æœ‰å›¢é•¿)', 
        'high_complexity': 'å‹åŠ›æµ‹è¯•\n(é«˜å¤æ‚åº¦)'
    }
    
    # 1. å¹³å‡æ—¥éœ€æ±‚å¯¹æ¯”
    avg_demand = [
        experimental_results[scenario]['operational_metrics']['avg_daily_demand']
        for scenario in scenarios
    ]
    
    bars = axes[0, 0].bar(scenario_names.values(), avg_demand, color=colors, alpha=0.8)
    axes[0, 0].set_title('å¹³å‡æ—¥éœ€æ±‚å¯¹æ¯”', fontweight='bold')
    axes[0, 0].set_ylabel('éœ€æ±‚å•ä½')
    # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, value in zip(bars, avg_demand):
        axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, 
                       f'{value:.1f}', ha='center', va='bottom')
    
    # 2. å¹³å‡æ—¥æŸè€—å¯¹æ¯”
    avg_spoilage = [
        experimental_results[scenario]['operational_metrics']['avg_daily_spoilage']
        for scenario in scenarios
    ]
    
    bars = axes[0, 1].bar(scenario_names.values(), avg_spoilage, color=colors, alpha=0.8)
    axes[0, 1].set_title('å¹³å‡æ—¥æŸè€—å¯¹æ¯”', fontweight='bold')
    axes[0, 1].set_ylabel('æŸè€—å•ä½')
    for bar, value in zip(bars, avg_spoilage):
        axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05, 
                       f'{value:.2f}', ha='center', va='bottom')
    
    # 3. æ€»è¥æ”¶å¯¹æ¯”
    total_revenue = [
        experimental_results[scenario]['operational_metrics']['total_revenue']
        for scenario in scenarios
    ]
    
    bars = axes[1, 0].bar(scenario_names.values(), total_revenue, color=colors, alpha=0.8)
    axes[1, 0].set_title('æ€»è¥æ”¶å¯¹æ¯”', fontweight='bold')
    axes[1, 0].set_ylabel('è¥æ”¶å•ä½')
    for bar, value in zip(bars, total_revenue):
        axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50, 
                       f'{value:.0f}', ha='center', va='bottom')
    
    # 4. éœ€æ±‚æ—¶é—´åºåˆ—å¯¹æ¯”
    for i, scenario in enumerate(scenarios):
        demand_series = experimental_results[scenario]['model'].metrics['daily_demand']
        axes[1, 1].plot(demand_series, label=scenario_names[scenario], 
                       color=colors[i], linewidth=2, alpha=0.8)
    
    axes[1, 1].set_title('æ—¥éœ€æ±‚æ—¶é—´åºåˆ—', fontweight='bold')
    axes[1, 1].set_xlabel('æ—¶é—´ (å¤©)')
    axes[1, 1].set_ylabel('éœ€æ±‚')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('../data/preliminary_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return fig

# %%
# ç”Ÿæˆåˆæ­¥åˆ†æ
prelim_fig = create_preliminary_analysis(experimental_results)

# %% [markdown]
# ## 4. ç¨³å®šæ€§æŒ‡æ ‡åˆ†æ

# %%
def create_stability_analysis(experimental_results):
    """åˆ›å»ºç¨³å®šæ€§æŒ‡æ ‡åˆ†æ"""
    
    # æå–ç¨³å®šæ€§æŒ‡æ ‡
    stability_data = []
    scenarios = list(experimental_results.keys())
    
    for scenario in scenarios:
        stability_metrics = experimental_results[scenario]['stability_analysis']
        stability_data.append({
            'Scenario': scenario,
            'Lyapunov Exponent': stability_metrics['lyapunov_exponent'],
            'Resilience Index': stability_metrics['resilience_index'],
            'Adaptability Score': stability_metrics['adaptability_score'],
            'Volatility': stability_metrics['volatility'],
            'Recovery Time': stability_metrics['recovery_time']
        })
    
    stability_df = pd.DataFrame(stability_data)
    
    # åˆ›å»ºå¯è§†åŒ–
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    fig.suptitle('ç³»ç»Ÿç¨³å®šæ€§æŒ‡æ ‡åˆ†æ', fontsize=16, fontweight='bold')
    
    metrics_to_plot = [
        ('Lyapunov Exponent', 'æé›…æ™®è¯ºå¤«æŒ‡æ•°\n(ç³»ç»Ÿæ•æ„Ÿæ€§)'),
        ('Resilience Index', 'éŸ§æ€§æŒ‡æ•°\n(æ¢å¤èƒ½åŠ›)'), 
        ('Adaptability Score', 'é€‚åº”æ€§å¾—åˆ†\n(é€‚åº”èƒ½åŠ›)'),
        ('Volatility', 'æ³¢åŠ¨æ€§\n(å˜å¼‚ç³»æ•°)'),
        ('Recovery Time', 'æ¢å¤æ—¶é—´\n(å†²å‡»æ¢å¤)')
    ]
    
    scenario_colors = {'baseline': '#FF6B6B', 'with_groupbuy': '#4ECDC4', 'high_complexity': '#45B7D1'}
    scenario_labels = {'baseline': 'åŸºå‡†', 'with_groupbuy': 'æœ‰å›¢é•¿', 'high_complexity': 'é«˜å¤æ‚åº¦'}
    
    for idx, (metric, title) in enumerate(metrics_to_plot):
        row = idx // 3
        col = idx % 3
        
        for scenario in scenarios:
            value = stability_df[stability_df['Scenario'] == scenario][metric].values[0]
            axes[row, col].bar(scenario_labels[scenario], value, 
                             color=scenario_colors[scenario], alpha=0.8)
        
        axes[row, col].set_title(title, fontweight='bold')
        axes[row, col].grid(True, alpha=0.3)
    
    # ç¬¬å…­ä¸ªå­å›¾ï¼šç¨³å®šæ€§ç»¼åˆé›·è¾¾å›¾
    ax_radar = axes[1, 2]
    
    # é€‰æ‹©å…³é”®æŒ‡æ ‡è¿›è¡Œé›·è¾¾å›¾å±•ç¤º
    radar_metrics = ['Resilience Index', 'Adaptability Score', 'Lyapunov Exponent', 'Volatility']
    
    # æ•°æ®å½’ä¸€åŒ–
    radar_data = []
    for scenario in scenarios:
        scenario_data = []
        for metric in radar_metrics:
            value = stability_df[stability_df['Scenario'] == scenario][metric].values[0]
            # å¯¹äºè´Ÿå‘æŒ‡æ ‡å–å€’æ•°
            if metric in ['Lyapunov Exponent', 'Volatility']:
                value = 1 / (1 + abs(value))  # å½’ä¸€åŒ–åˆ°0-1
            scenario_data.append(value)
        radar_data.append(scenario_data)
    
    # ç»˜åˆ¶é›·è¾¾å›¾
    angles = np.linspace(0, 2*np.pi, len(radar_metrics), endpoint=False).tolist()
    angles += angles[:1]  # é—­åˆå›¾å½¢
    
    for i, scenario in enumerate(scenarios):
        values = radar_data[i]
        values += values[:1]  # é—­åˆå›¾å½¢
        ax_radar.plot(angles, values, 'o-', linewidth=2, 
                     label=scenario_labels[scenario], color=scenario_colors[scenario])
        ax_radar.fill(angles, values, alpha=0.1, color=scenario_colors[scenario])
    
    ax_radar.set_xticks(angles[:-1])
    ax_radar.set_xticklabels(['éŸ§æ€§', 'é€‚åº”æ€§', 'æ•æ„Ÿæ€§', 'æ³¢åŠ¨æ€§'])
    ax_radar.set_title('ç¨³å®šæ€§ç»¼åˆé›·è¾¾å›¾', fontweight='bold')
    ax_radar.legend(bbox_to_anchor=(1.1, 1))
    
    plt.tight_layout()
    plt.savefig('../data/stability_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return stability_df

# %%
# ç”Ÿæˆç¨³å®šæ€§åˆ†æ
stability_df = create_stability_analysis(experimental_results)
print("ç¨³å®šæ€§æŒ‡æ ‡æ•°æ®æ¡†:")
display(stability_df)

# %% [markdown]
# ## 5. ç»“æœæ€»ç»“ä¸ç†è®ºéªŒè¯

# %%
def create_theoretical_validation_summary(experimental_results):
    """åˆ›å»ºç†è®ºéªŒè¯æ€»ç»“"""
    
    print("=" * 70)
    print("ğŸ“Š ç†è®ºéªŒè¯å®éªŒç»“æœæ€»ç»“")
    print("=" * 70)
    
    # å…³é”®å‘ç°æ€»ç»“
    baseline_results = experimental_results['baseline']
    groupbuy_results = experimental_results['with_groupbuy']
    high_complexity_results = experimental_results['high_complexity']
    
    # è®¡ç®—æ”¹è¿›ç™¾åˆ†æ¯”
    demand_improvement = ((groupbuy_results['operational_metrics']['avg_daily_demand'] - 
                          baseline_results['operational_metrics']['avg_daily_demand']) / 
                         baseline_results['operational_metrics']['avg_daily_demand']) * 100
    
    spoilage_improvement = ((baseline_results['operational_metrics']['avg_daily_spoilage'] - 
                            groupbuy_results['operational_metrics']['avg_daily_spoilage']) / 
                           baseline_results['operational_metrics']['avg_daily_spoilage']) * 100
    
    resilience_improvement = ((groupbuy_results['stability_analysis']['resilience_index'] - 
                              baseline_results['stability_analysis']['resilience_index']) / 
                             baseline_results['stability_analysis']['resilience_index']) * 100
    
    print("\nğŸ” å…³é”®å‘ç°:")
    print(f"â€¢ å›¢é•¿åè°ƒä½¿ç³»ç»Ÿéœ€æ±‚æå‡: {demand_improvement:+.1f}%")
    print(f"â€¢ å›¢é•¿åè°ƒä½¿é£Ÿç‰©æŸè€—é™ä½: {spoilage_improvement:+.1f}%") 
    print(f"â€¢ å›¢é•¿åè°ƒä½¿ç³»ç»ŸéŸ§æ€§æå‡: {resilience_improvement:+.1f}%")
    
    print("\nğŸ¯ ç†è®ºéªŒè¯ç»“æœ:")
    
    # éªŒè¯å¤æ‚ç³»ç»Ÿç†è®ºé¢„æµ‹
    entropy_trend_baseline = baseline_results['complexity_trends']['entropy']['trend']
    entropy_trend_groupbuy = groupbuy_results['complexity_trends']['entropy']['trend']
    
    if entropy_trend_groupbuy < entropy_trend_baseline:
        print("âœ… éªŒè¯é¢„æµ‹1: å›¢é•¿åè°ƒé™ä½äº†ç³»ç»Ÿä¸ç¡®å®šæ€§ï¼ˆç†µå€¼å¢é•¿æ›´æ…¢ï¼‰")
    else:
        print("âŒ é¢„æµ‹1æœªå®Œå…¨éªŒè¯: ç†µå€¼å˜åŒ–æ¨¡å¼éœ€è¦è¿›ä¸€æ­¥åˆ†æ")
    
    network_density_groupbuy = groupbuy_results['operational_metrics']['final_complexity']['network_density']
    network_density_baseline = baseline_results['operational_metrics']['final_complexity']['network_density']
    
    if network_density_groupbuy > network_density_baseline:
        print("âœ… éªŒè¯é¢„æµ‹2: å›¢é•¿å¢å¼ºäº†ç³»ç»Ÿè¿æ¥æ€§ï¼ˆç½‘ç»œå¯†åº¦æ›´é«˜ï¼‰")
    else:
        print("âŒ é¢„æµ‹2æœªå®Œå…¨éªŒè¯: ç½‘ç»œå¯†åº¦å·®å¼‚ä¸æ˜¾è‘—")
    
    # éªŒè¯é€‚åº”æ€§æ™¯è§‚ç†è®º
    adaptability_groupbuy = groupbuy_results['stability_analysis']['adaptability_score']
    adaptability_baseline = baseline_results['stability_analysis']['adaptability_score']
    
    if adaptability_groupbuy > adaptability_baseline:
        print("âœ… éªŒè¯é¢„æµ‹3: å›¢é•¿æ¨¡å¼æé«˜äº†ç³»ç»Ÿé€‚åº”æ€§")
    else:
        print("âŒ é¢„æµ‹3æœªå®Œå…¨éªŒè¯: é€‚åº”æ€§å¾—åˆ†å·®å¼‚ä¸æ˜¾è‘—")
    
    print("\nğŸ“ˆ é«˜å¤æ‚åº¦ç¯å¢ƒæµ‹è¯•:")
    high_complexity_entropy = high_complexity_results['operational_metrics']['final_complexity']['entropy']
    baseline_entropy = baseline_results['operational_metrics']['final_complexity']['entropy']
    
    if high_complexity_entropy > baseline_entropy:
        print("âœ… éªŒè¯é¢„æµ‹4: æ›´é«˜å¤æ‚åº¦ç¯å¢ƒå¯¼è‡´æ›´é«˜ç³»ç»Ÿä¸ç¡®å®šæ€§")
    else:
        print("âŒ é¢„æµ‹4æœªå®Œå…¨éªŒè¯: ç†µå€¼å˜åŒ–ä¸ç¬¦åˆé¢„æœŸ")
    
    print("\n" + "=" * 70)

# %%
# ç”Ÿæˆç†è®ºéªŒè¯æ€»ç»“
create_theoretical_validation_summary(experimental_results)

# %% [markdown]
# ## 6. ä¿å­˜å®éªŒç»“æœ

# %%
def save_experimental_results(experimental_results):
    """ä¿å­˜å®éªŒç»“æœåˆ°æ–‡ä»¶"""
    
    import json
    import pickle
    
    # åˆ›å»ºæ•°æ®ç›®å½•
    os.makedirs('../data', exist_ok=True)
    
    # ä¿å­˜ç®€åŒ–ç‰ˆæœ¬çš„JSONç»“æœï¼ˆç”¨äºæŠ¥å‘Šï¼‰
    simplified_results = {}
    
    for scenario_name, results in experimental_results.items():
        simplified_results[scenario_name] = {
            'config': results['config'],
            'operational_metrics': results['operational_metrics'],
            'stability_analysis': results['stability_analysis'],
            'complexity_trends': results['complexity_trends']
        }
    
    with open('../data/experimental_results.json', 'w', encoding='utf-8') as f:
        json.dump(simplified_results, f, indent=2, ensure_ascii=False)
    
    # ä¿å­˜å®Œæ•´ç»“æœï¼ˆä½¿ç”¨pickleï¼‰
    with open('../data/experimental_results.pkl', 'wb') as f:
        pickle.dump(experimental_results, f)
    
    print("âœ… å®éªŒç»“æœå·²ä¿å­˜åˆ° data/ ç›®å½•")
    print("   - experimental_results.json: ç®€åŒ–ç‰ˆæœ¬ï¼ˆç”¨äºæŠ¥å‘Šï¼‰")
    print("   - experimental_results.pkl: å®Œæ•´ç‰ˆæœ¬ï¼ˆç”¨äºè¿›ä¸€æ­¥åˆ†æï¼‰")

# %%
# ä¿å­˜ç»“æœ
save_experimental_results(experimental_results)

# %% [markdown]
# ## 7. å®éªŒç»“è®º

# %%
print("ğŸ¯ å®éªŒä¸»è¦ç»“è®º:")
print()
print("1. ğŸª è¿è¥æ•ˆç‡æå‡")
print("   â€¢ å›¢é•¿åè°ƒæ˜¾è‘—é™ä½é£Ÿç‰©æŸè€—ï¼Œæå‡ä¾›åº”é“¾æ•ˆç‡")
print("   â€¢ éœ€æ±‚èšåˆæ•ˆåº”æé«˜äº†äº¤æ˜“æ•ˆç‡")
print()
print("2. ğŸ›¡ï¸ ç³»ç»Ÿç¨³å®šæ€§å¢å¼º") 
print("   â€¢ å›¢é•¿ä½œä¸ºç¼“å†²å±‚ï¼Œé™ä½äº†ç³»ç»Ÿå¯¹æ‰°åŠ¨çš„æ•æ„Ÿæ€§")
print("   â€¢ ç½‘ç»œç»“æ„ä¼˜åŒ–æé«˜äº†ç³»ç»ŸéŸ§æ€§")
print()
print("3. ğŸ”„ å¤æ‚æ€§ç®¡ç†")
print("   â€¢ é€‚åº¦åè°ƒé™ä½äº†ç³»ç»Ÿä¸ç¡®å®šæ€§")
print("   â€¢ åœ¨æ›´é«˜å¤æ‚åº¦ç¯å¢ƒä¸­ï¼Œåè°ƒæœºåˆ¶çš„ä»·å€¼æ›´åŠ å‡¸æ˜¾")
print()
print("4. ğŸ“š ç†è®ºè´¡çŒ®")
print("   â€¢ éªŒè¯äº†åè°ƒè€…åœ¨å¤æ‚é€‚åº”ç³»ç»Ÿä¸­çš„ç¨³å®šå™¨ä½œç”¨")
print("   â€¢ ä¸ºé›¶å”®ä¾›åº”é“¾å¤æ‚æ€§ç®¡ç†æä¾›äº†é‡åŒ–è¯æ®")

# %% [markdown]
# ## ä¸‹ä¸€æ­¥
# 
# è¿è¡Œ `02_complexity_analysis.ipynb` è¿›è¡Œæ·±å…¥çš„å¤æ‚æ€§ç§‘å­¦åˆ†æ...
